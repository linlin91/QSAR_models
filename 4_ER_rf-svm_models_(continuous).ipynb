{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ER Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ER data set was collected from previous estrogen receptor binding studies and specifically refers to the chemical binding affinity of ERÎ±.\n",
    "\n",
    "\n",
    "reference:\n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3775906/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "endpoint values are ER-alpha log10 of 100*RBA (relative binding affinity vs estrogen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate rdkit continuous descriptors, splitting dataset, and descriptor preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "import pandas as pd\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "from rdkit.Chem import Descriptors\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "currentDirectory = os.getcwd()\n",
    "d = os.path.join(currentDirectory, \"Datasets\", \"ER_0801.csv\")\n",
    "dataset = pd.read_csv(d, index_col = 0)\n",
    "molecules = [Chem.MolFromSmiles(mol) for mol in dataset.SMILES]\n",
    "\n",
    "calculator = MoleculeDescriptors.MolecularDescriptorCalculator([desc[0] for desc in Descriptors.descList])\n",
    "X = pd.DataFrame([list(calculator.CalcDescriptors(mol)) for mol in molecules],\n",
    "                     index=dataset.index,\n",
    "                     columns=list(calculator.GetDescriptorNames()))\n",
    "\n",
    "train_set_X, test_set_X = train_test_split(X, test_size=0.2, random_state=42)\n",
    "train_set_y = dataset.loc[train_set_X.index]['endpoint'].values\n",
    "test_set_y = dataset.loc[test_set_X.index]['endpoint'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pipeline = pipeline.Pipeline([\n",
    "        ('scaling', MinMaxScaler()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "train_X_prepared = pipeline.fit_transform(train_set_X)\n",
    "test_X_prepared = pipeline.transform(test_set_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Hyperparameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# import numpy as np\n",
    "\n",
    "# # Number of trees in random forest\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 5, stop = 100, num = 5)]\n",
    "\n",
    "# # Number of features to consider at every split\n",
    "# max_features = ['auto', None]\n",
    "\n",
    "# # Maximum number of levels in tree\n",
    "# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "# max_depth.append(None)\n",
    "\n",
    "# # Minimum number of samples required to split a node\n",
    "# min_samples_split = [2, 5, 10]\n",
    "\n",
    "# # Minimum number of samples required at each leaf node\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# # Method of selecting samples for training each tree\n",
    "# bootstrap = [True, False]\n",
    "\n",
    "# # Create the random grid\n",
    "# random_grid = {'n_estimators': n_estimators,\n",
    "#                'max_features': max_features,\n",
    "#                'max_depth': max_depth,\n",
    "#                'min_samples_split': min_samples_split,\n",
    "#                'min_samples_leaf': min_samples_leaf,\n",
    "#                'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': [5, 28, 52, 76, 100],\n",
       " 'max_features': ['auto', None],\n",
       " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
       " 'min_samples_split': [2, 5, 10],\n",
       " 'min_samples_leaf': [1, 2, 4],\n",
       " 'bootstrap': [True, False]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   14.0s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   36.2s\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   51.0s finished\n",
      "D:\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestRegressor(bootstrap=True,\n",
       "                                                   criterion='mse',\n",
       "                                                   max_depth=None,\n",
       "                                                   max_features='auto',\n",
       "                                                   max_leaf_nodes=None,\n",
       "                                                   min_impurity_decrease=0.0,\n",
       "                                                   min_impurity_split=None,\n",
       "                                                   min_samples_leaf=1,\n",
       "                                                   min_samples_split=2,\n",
       "                                                   min_weight_fraction_leaf=0.0,\n",
       "                                                   n_estimators='warn',\n",
       "                                                   n_jobs=None, oob_score=False,\n",
       "                                                   random_sta...\n",
       "                                                   warm_start=False),\n",
       "                   iid='warn', n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', None],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [5, 28, 52, 76, 100]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# # Use the random grid to search for best hyperparameters\n",
    "# # First create the base model to tune\n",
    "\n",
    "# rf = RandomForestRegressor()\n",
    "\n",
    "# # Random search of parameters, using 5 fold cross validation, \n",
    "# # search across 100 different combinations, and use all available cores\n",
    "\n",
    "# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "# # Fit the random search model\n",
    "\n",
    "# rf_random.fit(train_X_prepared, train_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 100,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 30,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "MAE: 0.67.\n",
      "r2 = 0.75.\n",
      "Model Performance\n",
      "MAE: 0.66.\n",
      "r2 = 0.76.\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import r2_score\n",
    "# def evaluate(model, test_features, test_labels):\n",
    "#     predictions = model.predict(test_features)\n",
    "#     errors = abs(predictions - test_labels)\n",
    "#     MAE = np.mean(errors)\n",
    "#     r2 = r2_score(test_labels, predictions)\n",
    "#     print('Model Performance')\n",
    "#     print('MAE: {:0.2f}.'.format(MAE))\n",
    "#     print('r2 = {:0.2f}.'.format(r2))\n",
    "    \n",
    "#     return MAE\n",
    "\n",
    "\n",
    "# base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
    "# base_model.fit(train_X_prepared, train_set_y)\n",
    "# base_accuracy = evaluate(base_model, test_X_prepared, test_set_y)\n",
    "\n",
    "# best_random = rf_random.best_estimator_\n",
    "# random_accuracy = evaluate(best_random, test_X_prepared,  test_set_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    MAE = np.mean(errors)\n",
    "    r2 = r2_score(test_labels, predictions)\n",
    "    print('Model Performance')\n",
    "    print('MAE: {:0.2f}.'.format(MAE))\n",
    "    print('r2 = {:0.2f}.'.format(r2))\n",
    "    \n",
    "    return MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [5, 10, 20, 30, 40, 50],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5],\n",
    "    'min_samples_split': [2],\n",
    "    'n_estimators': [5, 100, 150, 200, 250, 300, 1000]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 210 candidates, totalling 1050 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   39.3s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1050 out of 1050 | elapsed:  6.8min finished\n",
      "D:\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 40,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 250}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(train_X_prepared, train_set_y)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "MAE: 0.66.\n",
      "r2 = 0.76.\n"
     ]
    }
   ],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "grid_accuracy = evaluate(best_grid, test_X_prepared, test_set_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save best model for future prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ER_rf_model_0806.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(best_grid, \"ER_rf_model_0806.pkl\")\n",
    "#my_model_loaded = joblib.load(\"my_model.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'kernel': ['rbf'],\n",
    "    'gamma': [1e-2, 1e-3],\n",
    "    'C': [1,10]}\n",
    "# Create a based model\n",
    "svm = SVR()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = svm, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  20 | elapsed:    1.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    1.1s finished\n",
      "D:\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(train_X_prepared, train_set_y)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "MAE: 0.62.\n",
      "r2 = 0.79.\n"
     ]
    }
   ],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "grid_accuracy = evaluate(best_grid, test_X_prepared, test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ER_svm_model_0806.pkl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(best_grid, \"ER_svm_model_0806.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
